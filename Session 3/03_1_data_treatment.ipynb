{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Libraries and datasets, and define global variables"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T17:32:58.219026Z",
     "start_time": "2026-01-19T17:32:58.203339Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "INPUT_PATH = \"data_in\"\n",
    "OUTPUT_PATH = \"data_out\"\n",
    "\n",
    "ID_LABELS = [\"SK_ID_CURR\"]\n",
    "TARGET_LABEL = \"TARGET\"\n",
    "\n",
    "TRAIN_FRAC = 0.8\n",
    "VAL_FRAC = 0.1\n",
    "TEST_FRAC = 0.1\n",
    "\n",
    "HIGHLY_MISSING_THRESHOLD = 0.6\n",
    "HIGHLY_CONCENTRATED_THRESHOLD = 0.9"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create helper classes & functions"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T17:32:58.235537Z",
     "start_time": "2026-01-19T17:32:58.225567Z"
    }
   },
   "source": [
    "def get_missing_percentage(df):\n",
    "    \"\"\"Calculate the percentage of missing values for each column.\"\"\"\n",
    "    missing_pct = df.isnull().mean()\n",
    "    missing_df = pd.DataFrame({\n",
    "        'variable': missing_pct.index,\n",
    "        'perc_missing': missing_pct.values\n",
    "    })\n",
    "    return missing_df.sort_values('perc_missing', ascending=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def get_concentration(df, exclude_cols=None):\n",
    "    \"\"\"Calculate the concentration (max frequency) for each column.\"\"\"\n",
    "    if exclude_cols is None:\n",
    "        exclude_cols = []\n",
    "    \n",
    "    results = []\n",
    "    for col in df.columns:\n",
    "        if col in exclude_cols:\n",
    "            continue\n",
    "        # Calculate value counts normalized\n",
    "        vc = df[col].value_counts(normalize=True, dropna=False)\n",
    "        max_conc = vc.iloc[0] if len(vc) > 0 else 0\n",
    "        most_common_vals = vc[vc == max_conc].index.tolist()\n",
    "        results.append({\n",
    "            'variable': col,\n",
    "            'values_with_most_concentration': most_common_vals,\n",
    "            'concentration': max_conc\n",
    "        })\n",
    "    \n",
    "    result_df = pd.DataFrame(results)\n",
    "    return result_df.sort_values('concentration', ascending=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def print_dataset_info(df, name, target_col=None):\n",
    "    \"\"\"Print dataset size and target rate.\"\"\"\n",
    "    if target_col and target_col in df.columns:\n",
    "        target_rate = df[target_col].mean()\n",
    "        print(f\"{name} size = {df.shape}. Target rate = {target_rate}.\")\n",
    "    else:\n",
    "        print(f\"{name} size = {df.shape}. Target rate = nan\")\n",
    "\n",
    "\n",
    "def convert_to_numeric(df, exclude_cols=None):\n",
    "    \"\"\"\n",
    "    Convert all columns to numeric where possible.\n",
    "    - Boolean columns are converted to 0/1\n",
    "    - Object columns with numeric-like values are converted to numeric\n",
    "    - 'True'/'False' strings are converted to 1/0\n",
    "    \"\"\"\n",
    "    if exclude_cols is None:\n",
    "        exclude_cols = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if col in exclude_cols:\n",
    "            continue\n",
    "        \n",
    "        # Convert bool columns to int\n",
    "        if df[col].dtype == 'bool':\n",
    "            df[col] = df[col].astype(int)\n",
    "        \n",
    "        # Convert object columns\n",
    "        elif df[col].dtype == 'object':\n",
    "            # First, replace 'True'/'False' strings with 1/0\n",
    "            df[col] = df[col].replace({'True': 1, 'False': 0})\n",
    "            # Then convert to numeric (coerce will turn unconvertible to NaN)\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Import data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2026-01-19T17:32:58.733398Z",
     "start_time": "2026-01-19T17:32:58.243652Z"
    }
   },
   "source": [
    "# Filename: \"data_extraction.csv\"\n",
    "df = pd.read_csv(f\"{INPUT_PATH}/data_extraction.csv\")\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (29718, 315)\n",
      "\n",
      "First few rows:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "   SK_ID_CURR  TARGET  CNT_CHILDREN  AMT_INCOME_TOTAL  AMT_CREDIT  \\\n",
       "0      100004     0.0             0           67500.0    135000.0   \n",
       "1      100012     0.0             0          135000.0    405000.0   \n",
       "2      100021     0.0             1           81000.0    270000.0   \n",
       "3      100022     0.0             0          112500.0    157500.0   \n",
       "4      100024     0.0             0          135000.0    427500.0   \n",
       "\n",
       "   AMT_ANNUITY_x  AMT_GOODS_PRICE  REGION_POPULATION_RELATIVE  DAYS_BIRTH  \\\n",
       "0         6750.0         135000.0                    0.010032      -19046   \n",
       "1        20250.0         405000.0                    0.019689      -14469   \n",
       "2        13500.0         270000.0                    0.010966       -9776   \n",
       "3         7875.0         157500.0                    0.046220      -17718   \n",
       "4        21375.0         427500.0                    0.015221      -18252   \n",
       "\n",
       "   DAYS_EMPLOYED  ...  NUM_TIMES_12M_STATUS_0  \\\n",
       "0           -225  ...                     NaN   \n",
       "1          -2019  ...                     NaN   \n",
       "2           -191  ...                     NaN   \n",
       "3          -7804  ...                     0.0   \n",
       "4          -4286  ...                     NaN   \n",
       "\n",
       "   NUM_TIMES_12M_IS_MOST_RECENT_STATUS_1  NUM_TIMES_12M_STATUS_1  \\\n",
       "0                                    NaN                     NaN   \n",
       "1                                    NaN                     NaN   \n",
       "2                                    NaN                     NaN   \n",
       "3                                      0                     0.0   \n",
       "4                                    NaN                     NaN   \n",
       "\n",
       "   NUM_TIMES_12M_IS_MOST_RECENT_STATUS_C  NUM_TIMES_12M_STATUS_C  \\\n",
       "0                                    NaN                     NaN   \n",
       "1                                    NaN                     NaN   \n",
       "2                                    NaN                     NaN   \n",
       "3                                      0                     0.0   \n",
       "4                                    NaN                     NaN   \n",
       "\n",
       "   NUM_TIMES_12M_IS_MOST_RECENT_STATUS_X  NUM_TIMES_12M_STATUS_X  \\\n",
       "0                                    NaN                     NaN   \n",
       "1                                    NaN                     NaN   \n",
       "2                                    NaN                     NaN   \n",
       "3                                      0                     0.0   \n",
       "4                                    NaN                     NaN   \n",
       "\n",
       "   NUM_TIMES_12M_IS_MOST_RECENT_STATUS_nan  NUM_TIMES_12M_STATUS_nan  \\\n",
       "0                                      NaN                       NaN   \n",
       "1                                      NaN                       NaN   \n",
       "2                                      NaN                       NaN   \n",
       "3                                        0                       0.0   \n",
       "4                                      NaN                       NaN   \n",
       "\n",
       "   IS_CREDIT_ENDDATE_MISSING  \n",
       "0                        NaN  \n",
       "1                        NaN  \n",
       "2                        NaN  \n",
       "3                        0.0  \n",
       "4                        NaN  \n",
       "\n",
       "[5 rows x 315 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>CNT_CHILDREN</th>\n",
       "      <th>AMT_INCOME_TOTAL</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>AMT_ANNUITY_x</th>\n",
       "      <th>AMT_GOODS_PRICE</th>\n",
       "      <th>REGION_POPULATION_RELATIVE</th>\n",
       "      <th>DAYS_BIRTH</th>\n",
       "      <th>DAYS_EMPLOYED</th>\n",
       "      <th>...</th>\n",
       "      <th>NUM_TIMES_12M_STATUS_0</th>\n",
       "      <th>NUM_TIMES_12M_IS_MOST_RECENT_STATUS_1</th>\n",
       "      <th>NUM_TIMES_12M_STATUS_1</th>\n",
       "      <th>NUM_TIMES_12M_IS_MOST_RECENT_STATUS_C</th>\n",
       "      <th>NUM_TIMES_12M_STATUS_C</th>\n",
       "      <th>NUM_TIMES_12M_IS_MOST_RECENT_STATUS_X</th>\n",
       "      <th>NUM_TIMES_12M_STATUS_X</th>\n",
       "      <th>NUM_TIMES_12M_IS_MOST_RECENT_STATUS_nan</th>\n",
       "      <th>NUM_TIMES_12M_STATUS_nan</th>\n",
       "      <th>IS_CREDIT_ENDDATE_MISSING</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>67500.0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>6750.0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>0.010032</td>\n",
       "      <td>-19046</td>\n",
       "      <td>-225</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>405000.0</td>\n",
       "      <td>20250.0</td>\n",
       "      <td>405000.0</td>\n",
       "      <td>0.019689</td>\n",
       "      <td>-14469</td>\n",
       "      <td>-2019</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100021</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>81000.0</td>\n",
       "      <td>270000.0</td>\n",
       "      <td>13500.0</td>\n",
       "      <td>270000.0</td>\n",
       "      <td>0.010966</td>\n",
       "      <td>-9776</td>\n",
       "      <td>-191</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100022</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>112500.0</td>\n",
       "      <td>157500.0</td>\n",
       "      <td>7875.0</td>\n",
       "      <td>157500.0</td>\n",
       "      <td>0.046220</td>\n",
       "      <td>-17718</td>\n",
       "      <td>-7804</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100024</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>427500.0</td>\n",
       "      <td>21375.0</td>\n",
       "      <td>427500.0</td>\n",
       "      <td>0.015221</td>\n",
       "      <td>-18252</td>\n",
       "      <td>-4286</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 315 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Step 1: Treat Boolean and categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2026-01-19T17:32:58.832792Z",
     "start_time": "2026-01-19T17:32:58.746134Z"
    }
   },
   "source": [
    "# Check if some categorical variables are actually numeric, remove true categorical variables, and\n",
    "# convert Boolean variables to binary.\n",
    "\n",
    "# Identify column types\n",
    "print(\"Data types distribution (before treatment):\")\n",
    "print(df.dtypes.value_counts())\n",
    "\n",
    "# Identify boolean columns (True/False)\n",
    "bool_cols = df.select_dtypes(include=['bool']).columns.tolist()\n",
    "print(f\"\\nBoolean columns: {len(bool_cols)}\")\n",
    "\n",
    "# Identify object (string/categorical) columns\n",
    "object_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"Object/categorical columns: {len(object_cols)}\")\n",
    "\n",
    "# Convert all non-ID columns to numeric\n",
    "df = convert_to_numeric(df, exclude_cols=ID_LABELS)\n",
    "\n",
    "print(f\"\\nData types distribution (after treatment):\")\n",
    "print(df.dtypes.value_counts())\n",
    "\n",
    "# Check remaining object columns (these would be true categorical)\n",
    "remaining_object_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "remaining_object_cols = [c for c in remaining_object_cols if c not in ID_LABELS]\n",
    "\n",
    "if remaining_object_cols:\n",
    "    print(f\"\\nRemoving {len(remaining_object_cols)} true categorical columns: {remaining_object_cols}\")\n",
    "    df = df.drop(columns=remaining_object_cols)\n",
    "\n",
    "print(f\"\\nDataset shape after treatment: {df.shape}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types distribution (before treatment):\n",
      "bool       155\n",
      "float64    114\n",
      "int64       41\n",
      "object       5\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Boolean columns: 155\n",
      "Object/categorical columns: 5\n",
      "\n",
      "Data types distribution (after treatment):\n",
      "int64      196\n",
      "float64    119\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Dataset shape after treatment: (29718, 315)\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Step 2: Split to impact, training, validation, and testing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2026-01-19T17:32:59.102076Z",
     "start_time": "2026-01-19T17:32:58.839999Z"
    }
   },
   "source": [
    "# Include stratification on the target: \n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "# Impact records are those that have the flag \"is_test\" equals to 1. It doesn't have a target value.\n",
    "\n",
    "# Print extraction info\n",
    "print_dataset_info(df, \"Extraction\", TARGET_LABEL)\n",
    "\n",
    "# Separate impact records (is_test == 1) from development records\n",
    "df_impact = df[df['is_test'] == 1].drop(columns=['is_test']).reset_index(drop=True)\n",
    "df_dev = df[df['is_test'] == 0].drop(columns=['is_test']).reset_index(drop=True)\n",
    "\n",
    "# Get development target for stratification\n",
    "y_dev = df_dev[TARGET_LABEL]\n",
    "\n",
    "# First split: train + val vs test (using remaining fraction)\n",
    "# TRAIN_FRAC is 0.8 of total dev, so test is 0.1/0.9 of remaining\n",
    "test_size = TEST_FRAC / (TRAIN_FRAC + VAL_FRAC + TEST_FRAC)\n",
    "\n",
    "df_train_val, df_test = train_test_split(\n",
    "    df_dev,\n",
    "    test_size=test_size,\n",
    "    stratify=y_dev,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Second split: train vs val\n",
    "val_size = VAL_FRAC / (TRAIN_FRAC + VAL_FRAC)\n",
    "\n",
    "df_train, df_val = train_test_split(\n",
    "    df_train_val,\n",
    "    test_size=val_size,\n",
    "    stratify=df_train_val[TARGET_LABEL],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Reset indices for clean dataframes\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "\n",
    "# Print info for all splits\n",
    "print_dataset_info(df_train, \"Train\", TARGET_LABEL)\n",
    "print_dataset_info(df_val, \"Val\", TARGET_LABEL)\n",
    "print_dataset_info(df_test, \"Test\", TARGET_LABEL)\n",
    "print_dataset_info(df_impact, \"Impact\", TARGET_LABEL)\n",
    "\n",
    "# Output example.\n",
    "# Extraction size = (29718, 315). Target rate = 0.05478329177909082.\n",
    "# Train size = (23423, 314). Target rate = 0.054775220936686166.\n",
    "# Val size = (2928, 314). Target rate = 0.054986338797814206.\n",
    "# Test size = (2928, 314). Target rate = 0.0546448087431694\n",
    "# Impact size = (439, 314). Target rate = nan"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction size = (29718, 315). Target rate = 0.05478329177909082.\n",
      "Train size = (23423, 314). Target rate = 0.05481791401613798.\n",
      "Val size = (2928, 314). Target rate = 0.0546448087431694.\n",
      "Test size = (2928, 314). Target rate = 0.0546448087431694.\n",
      "Impact size = (439, 314). Target rate = nan.\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Step 3: Treat missing values (based on training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1. Remove features at or above highly_missing_threshold"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T17:32:59.173271Z",
     "start_time": "2026-01-19T17:32:59.108996Z"
    }
   },
   "source": [
    "# Calculate missing percentages on training data (excluding ID and target)\n",
    "feature_cols = [col for col in df_train.columns if col not in ID_LABELS + [TARGET_LABEL]]\n",
    "missing_df = get_missing_percentage(df_train[feature_cols])\n",
    "\n",
    "# Find features with missing rate >= threshold\n",
    "highly_missing = missing_df[missing_df['perc_missing'] >= HIGHLY_MISSING_THRESHOLD]\n",
    "\n",
    "print(\"Features with high missing rates:\")\n",
    "print(highly_missing.head().to_string())\n",
    "\n",
    "# Get list of columns to remove\n",
    "cols_to_remove = highly_missing['variable'].tolist()\n",
    "\n",
    "# Remove from all datasets\n",
    "df_train = df_train.drop(columns=cols_to_remove)\n",
    "df_val = df_val.drop(columns=cols_to_remove)\n",
    "df_test = df_test.drop(columns=cols_to_remove)\n",
    "df_impact = df_impact.drop(columns=cols_to_remove)\n",
    "\n",
    "print(f\"\\nSize of train after removal = {df_train.shape}.\")\n",
    "print(f\"# features removed = {len(cols_to_remove)}.\")\n",
    "\n",
    "# Output example:\n",
    "# variable \tperc_missing\n",
    "# 0 \tCOMMONAREA_AVG \t0.668830\n",
    "# 1 \tCOMMONAREA_MEDI \t0.668830\n",
    "# 2 \tCOMMONAREA_MODE \t0.668830\n",
    "# 3 \tNONLIVINGAPARTMENTS_AVG \t0.662938\n",
    "# 4 \tNONLIVINGAPARTMENTS_MEDI \t0.662938\n",
    "\n",
    "# Size of train after removal = (23423, 298).\n",
    "# # features removed = 16."
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with high missing rates:\n",
      "                   variable  perc_missing\n",
      "0            COMMONAREA_AVG      0.670196\n",
      "1           COMMONAREA_MEDI      0.670196\n",
      "2           COMMONAREA_MODE      0.670196\n",
      "3  NONLIVINGAPARTMENTS_MEDI      0.664902\n",
      "4   NONLIVINGAPARTMENTS_AVG      0.664902\n",
      "\n",
      "Size of train after removal = (23423, 298).\n",
      "# features removed = 16.\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2. Create a binary flag for every variable that has a missing value"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T17:32:59.288892Z",
     "start_time": "2026-01-19T17:32:59.185112Z"
    }
   },
   "source": [
    "# Identify columns with missing values in training set (excluding ID and target)\n",
    "feature_cols = [col for col in df_train.columns if col not in ID_LABELS + [TARGET_LABEL]]\n",
    "\n",
    "# Find columns with any missing values\n",
    "cols_with_missing = [col for col in feature_cols if df_train[col].isnull().any()]\n",
    "\n",
    "print(f\"Number of columns with missing values: {len(cols_with_missing)}\")\n",
    "\n",
    "# Create binary flags for each column with missing values\n",
    "for col in cols_with_missing:\n",
    "    flag_col_name = f\"{col}_missing\"\n",
    "    df_train[flag_col_name] = df_train[col].isnull().astype(int)\n",
    "    df_val[flag_col_name] = df_val[col].isnull().astype(int)\n",
    "    df_test[flag_col_name] = df_test[col].isnull().astype(int)\n",
    "    df_impact[flag_col_name] = df_impact[col].isnull().astype(int)\n",
    "\n",
    "print(f\"Size of train after adding missing flags = {df_train.shape}.\")\n",
    "print(f\"# features added = {len(cols_with_missing)}.\")\n",
    "\n",
    "# Output example:\n",
    "# Size of train after removal = (23423, 393).\n",
    "# # features added = 95."
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns with missing values: 95\n",
      "Size of train after adding missing flags = (23423, 393).\n",
      "# features added = 95.\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.3. Impute missing values with median value and apply results on validation and testing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T17:33:00.235395Z",
     "start_time": "2026-01-19T17:32:59.304323Z"
    }
   },
   "source": [
    "# checkout https://scikit-learn.org/stable/modules/impute.html\n",
    "\n",
    "# Get current feature columns (excluding ID and target)\n",
    "feature_cols = [col for col in df_train.columns if col not in ID_LABELS + [TARGET_LABEL]]\n",
    "\n",
    "# Create imputer based on training data\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "remaining_missing = get_missing_percentage(df_train[feature_cols])\n",
    "print(remaining_missing.head())\n",
    "# Fit on training data and transform all datasets\n",
    "df_train[feature_cols] = imputer.fit_transform(df_train[feature_cols])\n",
    "df_val[feature_cols] = imputer.transform(df_val[feature_cols])\n",
    "df_test[feature_cols] = imputer.transform(df_test[feature_cols])\n",
    "df_impact[feature_cols] = imputer.transform(df_impact[feature_cols])\n",
    "\n",
    "# Verify no missing values remain\n",
    "remaining_missing = get_missing_percentage(df_train[feature_cols])\n",
    "remaining_missing = remaining_missing[remaining_missing['perc_missing'] > 0]\n",
    "\n",
    "if len(remaining_missing) > 0:\n",
    "    print(\"Columns still with missing values after imputation:\")\n",
    "    print(remaining_missing.head())\n",
    "else:\n",
    "    print(\"All missing values have been imputed successfully.\")\n",
    "\n",
    "print(f\"\\nTrain shape after imputation: {df_train.shape}\")\n",
    "\n",
    "# Output example:\n",
    "# variable \tperc_missing\n",
    "# 0 \tNUM_TIMES_12M_IS_MOST_RECENT_STATUS_0 \t0.502455\n",
    "# 1 \tNUM_TIMES_12M_IS_MOST_RECENT_STATUS_1 \t0.502455\n",
    "# 2 \tNUM_TIMES_12M_IS_MOST_RECENT_STATUS_C \t0.502455\n",
    "# 3 \tNUM_TIMES_12M_IS_MOST_RECENT_STATUS_X \t0.502455\n",
    "# 4 \tNUM_TIMES_12M_IS_MOST_RECENT_STATUS_nan \t0.502455"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            variable  perc_missing\n",
      "0       LANDAREA_AVG      0.554156\n",
      "1      LANDAREA_MEDI      0.554156\n",
      "2      LANDAREA_MODE      0.554156\n",
      "3  BASEMENTAREA_MEDI      0.544508\n",
      "4   BASEMENTAREA_AVG      0.544508\n",
      "All missing values have been imputed successfully.\n",
      "\n",
      "Train shape after imputation: (23423, 393)\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. Step 4: Remove features below highly_concentrated_threshold (based on training)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T17:33:00.481371Z",
     "start_time": "2026-01-19T17:33:00.243446Z"
    }
   },
   "source": [
    "# Get current feature columns (excluding ID and target)\n",
    "feature_cols = [col for col in df_train.columns if col not in ID_LABELS + [TARGET_LABEL]]\n",
    "\n",
    "# Calculate concentration for each feature\n",
    "concentration_df = get_concentration(df_train[feature_cols])\n",
    "\n",
    "# Find features with concentration >= threshold\n",
    "highly_concentrated = concentration_df[\n",
    "    concentration_df['concentration'] >= HIGHLY_CONCENTRATED_THRESHOLD\n",
    "]\n",
    "\n",
    "print(\"Features with high concentration:\")\n",
    "print(highly_concentrated.head().to_string())\n",
    "\n",
    "# Get list of columns to remove\n",
    "cols_to_remove_concentrated = highly_concentrated['variable'].tolist()\n",
    "\n",
    "# Remove from all datasets\n",
    "df_train = df_train.drop(columns=cols_to_remove_concentrated)\n",
    "df_val = df_val.drop(columns=cols_to_remove_concentrated)\n",
    "df_test = df_test.drop(columns=cols_to_remove_concentrated)\n",
    "df_impact = df_impact.drop(columns=cols_to_remove_concentrated)\n",
    "\n",
    "print(f\"\\nSize of train after removal = {df_train.shape}.\")\n",
    "print(f\"# features removed = {len(cols_to_remove_concentrated)}.\")\n",
    "\n",
    "# Output example:\n",
    "# variable \tvalues_with_most_concentration \tconcentration\n",
    "# 0 \tCODE_GENDER_nan \t[0.0] \t1.0\n",
    "# 1 \tFLAG_DOCUMENT_12 \t[0.0] \t1.0\n",
    "# 2 \tFLAG_DOCUMENT_2 \t[0.0] \t1.0\n",
    "# 3 \tFLAG_DOCUMENT_20 \t[0.0] \t1.0\n",
    "# 4 \tFLAG_MOBIL \t[1.0] \t1.0\n",
    "\n",
    "# Size of train after removal = (23423, 202).\n",
    "# # features removed = 191."
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with high concentration:\n",
      "                                     variable values_with_most_concentration  concentration\n",
      "0     NUM_TIMES_12M_IS_MOST_RECENT_STATUS_nan                          [0.0]            1.0\n",
      "1                    NUM_TIMES_12M_STATUS_nan                          [0.0]            1.0\n",
      "2           NUM_TIMES_12M_CREDIT_CURRENCY_nan                          [0.0]            1.0\n",
      "3             NUM_TIMES_12M_CREDIT_ACTIVE_nan                          [0.0]            1.0\n",
      "4  NUM_TIMES_12M_CREDIT_TYPE_Real estate loan                          [0.0]            1.0\n",
      "\n",
      "Size of train after removal = (23423, 202).\n",
      "# features removed = 191.\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Store final treated datasets"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T17:33:02.687240Z",
     "start_time": "2026-01-19T17:33:00.488745Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "# Save treated datasets\n",
    "df_train.to_csv(f\"{OUTPUT_PATH}/data_treatment_train.csv\", index=False)\n",
    "df_val.to_csv(f\"{OUTPUT_PATH}/data_treatment_val.csv\", index=False)\n",
    "df_test.to_csv(f\"{OUTPUT_PATH}/data_treatment_test.csv\", index=False)\n",
    "df_impact.to_csv(f\"{OUTPUT_PATH}/data_treatment_impact.csv\", index=False)\n",
    "\n",
    "print(\"Saved treated datasets to:\")\n",
    "print(f\"  - {OUTPUT_PATH}/data_treatment_train.csv\")\n",
    "print(f\"  - {OUTPUT_PATH}/data_treatment_val.csv\")\n",
    "print(f\"  - {OUTPUT_PATH}/data_treatment_test.csv\")\n",
    "print(f\"  - {OUTPUT_PATH}/data_treatment_impact.csv\")\n",
    "\n",
    "print(\"\\nFinal dataset shapes:\")\n",
    "print(f\"  Train: {df_train.shape}\")\n",
    "print(f\"  Val: {df_val.shape}\")\n",
    "print(f\"  Test: {df_test.shape}\")\n",
    "print(f\"  Impact: {df_impact.shape}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved treated datasets to:\n",
      "  - data_out/data_treatment_train.csv\n",
      "  - data_out/data_treatment_val.csv\n",
      "  - data_out/data_treatment_test.csv\n",
      "  - data_out/data_treatment_impact.csv\n",
      "\n",
      "Final dataset shapes:\n",
      "  Train: (23423, 202)\n",
      "  Val: (2928, 202)\n",
      "  Test: (2928, 202)\n",
      "  Impact: (439, 202)\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-19T17:33:02.695318Z",
     "start_time": "2026-01-19T17:33:02.693126Z"
    }
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
